# ig_explanation.py (DistilBERT适配版)
!pip install -q transformers shap alibi

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from alibi.explainers import IntegratedGradients

# 加载预训练模型与分词器
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# 语言学基线生成函数
def create_linguistic_baseline(text):
    encoded = tokenizer(text, return_tensors='tf', padding='max_length', max_length=50)
    baseline_ids = np.full_like(encoded['input_ids'], tokenizer.mask_token_id)  # 使用[MASK]基线
    return {'input_ids': baseline_ids, 'attention_mask': encoded['attention_mask']}

# IG解释器配置
def ig_explain(text):
    # 编码文本
    inputs = tokenizer(text, return_tensors='tf', padding='max_length', max_length=50)
    baseline = create_linguistic_baseline(text)
    
    # 初始化IG
    ig = IntegratedGradients(
        model, 
        layer=model.distilbert.embeddings,  # 直接定位嵌入层
        n_steps=20
    )
    
    # 计算归因
    explanation = ig.explain(
        inputs={'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},
        baselines=baseline
    )
    
    # 可视化处理
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    attrs = explanation.attributions[0][0].numpy().sum(axis=1)  # 聚合嵌入维度
    
    # 可视化
    tokens = [reverse_index.get(idx, 'UNK') for idx in sample_vec if idx != 0]
    plt.figure(figsize=(10, 3))
    plt.barh(tokens, attrs[:len(tokens)], color='purple')
    plt.title("Integrated Gradients Attribution")
    plt.show()

# 测试案例
if __name__ == "__main__":
    test_sentence = (
        "This film is disturbing but sincere. It's worth the time "
        "though difficult to find in stores."
    )
    ig_explain(test_sentence)
