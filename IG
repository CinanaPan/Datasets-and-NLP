# ig_explanation.py (DistilBERT适配版)
!pip install -q transformers shap alibi

import numpy as np
import tensorflow as tf
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from alibi.explainers import IntegratedGradients

# 加载预训练模型与分词器
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# 语言学基线生成函数
def create_linguistic_baseline(text):
    encoded = tokenizer(text, return_tensors='tf', padding='max_length', max_length=50)
    baseline_ids = np.full_like(encoded['input_ids'], tokenizer.mask_token_id)  # 使用[MASK]基线
    return {'input_ids': baseline_ids, 'attention_mask': encoded['attention_mask']}

# IG解释器配置
def ig_explain(text):
    # 编码文本
    inputs = tokenizer(text, return_tensors='tf', padding='max_length', max_length=50)
    baseline = create_linguistic_baseline(text)
    
    # 初始化IG
    ig = IntegratedGradients(
        model, 
        layer=model.distilbert.embeddings,  # 直接定位嵌入层
        n_steps=20
    )
    
    # 计算归因
    explanation = ig.explain(
        inputs={'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},
        baselines=baseline
    )
    
    # 可视化处理
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    attrs = explanation.attributions[0][0].numpy().sum(axis=1)  # 聚合嵌入维度
    
    # 绘制归因图（具体绘图代码与原文类似）
